\chapter{Neural Network Configuration}
\label{chapter:hyper}
\section{Hyperparameter Optimization I}

Fig.\,\ref{fig:hyperband_resnet_params} provides a summary of the search space for the initial hyperparameter tuning process and its results. These parameters are critical as they influence the model's learning process, generalization ability, and susceptibility to issues such as overfitting or bias. The chosen values in the search space represent a balance between exploring a wide range of options and focusing on promising areas based on prior knowledge or domain-specific considerations.

\begin{figure}[ht]
    \centering
    \includegraphics[width=0.9\linewidth, trim={10pt 10pt 15pt 40pt}, clip]{figures/figures_tuner/hyperband_resnet_params.pdf}
    \caption{The top f2-scores for the initial hyperparameter tuning process are highlighted, with the best score marked by a diamond shape.}
    \label{fig:hyperband_resnet_params}
\end{figure}

The most successful trial values are marked by a diamond shape in Fig.\,\ref{fig:hyperband_resnet_params}. While some of the initial trial's hyperparameters do not show significant overall improvement, some parameters such as class weight, training years, and kernel regularizer do stand out. 

\section{Hyperparameter Optimization II}

A second hyperparameter tuning process was performed using the best values from the most successful trial. This second set of trials looked into the number of layers, the filter size of the convolutional layers, and the number of units in the fully-connected (dense) layers. 

\begin{figure}[ht]
    \centering
    \includegraphics[width=0.9\linewidth, trim={10pt 10pt 15pt 40pt}, clip]{figures/figures_tuner/hyperband_resnet_followup_params.pdf}
    \caption{The top f2-scores for the initial hyperparameter tuning process are highlighted, with the best score marked by a diamond shape.}
    \label{fig:hyperband_resnet_followup_params}
\end{figure}

\section{Model Architecture}

\begin{figure}[ht]
    \centering
    \includegraphics[width=0.9\linewidth]{figures/figures_tuner/model_layered_view.png}
    \caption{The model's layered architecture is depicted. The top-right shows the input layers, the middle section displays the convolutional layers, and the bottom-right illustrates the fully-connected layers. The layered views were generated using VisualKeras (\cite{visualkeras}).}
    \label{fig:model_layered_view}
\end{figure}

Fig.\,\ref{fig:model_layered_view} illustrates the detailed architecture of the deep learning model built using the results of the hyperparameter tuning processes. The figure highlights the flow of data through its various layers. The model starts with input layers, which include upsampling layers to handle inputs with different dimensions. This ensures that all input sources are properly aligned for processing within the model. The network then progresses through multiple convolutional layers, with filter sizes generally increasing as the layers deepen. Batch normalization and dropout layers are strategically placed to regularize the model and prevent overfitting.

The architecture incorporates residual connections for maintaining gradient flow during backpropagation. These connections enable the network to be deep without suffering from issues like vanishing gradients, as highlighted in \cite{resnet}. As the network advances, spatial dimensions are reduced through pooling and striding operations, leading to a flattening layer that prepares the data for the dense layers. These dense layers further reduce the dimensionality, eventually outputting a 7-dimensional vector corresponding to the classification into 7 different tree genera, aligning with the model's goal of predicting tree types.



