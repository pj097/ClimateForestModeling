{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2e74685f-58ca-4865-8f90-241cb459f2f6",
   "metadata": {},
   "source": [
    "### source\n",
    "\n",
    "https://towardsdatascience.com/solving-differential-equations-with-neural-networks-afdcf7b8bcc4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4522011-24db-4a72-bf1e-ddc5b2942dbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearNN(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_inputs: int = 1,\n",
    "        num_layers: int = 1,\n",
    "        num_neurons: int = 5,\n",
    "        act: nn.Module = nn.Tanh(),\n",
    "    ) -> None:\n",
    "        \"\"\"Basic neural network architecture with linear layers\n",
    "        \n",
    "        Args:\n",
    "            num_inputs (int, optional): the dimensionality of the input tensor\n",
    "            num_layers (int, optional): the number of hidden layers\n",
    "            num_neurons (int, optional): the number of neurons for each hidden layer\n",
    "            act (nn.Module, optional): the non-linear activation function to use for stitching\n",
    "                linear layers togeter\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        \n",
    "        self.num_inputs = num_inputs\n",
    "        self.num_neurons = num_neurons\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        layers = []\n",
    "\n",
    "        # input layer\n",
    "        layers.append(nn.Linear(self.num_inputs, num_neurons))\n",
    "\n",
    "        # hidden layers with linear layer and activation\n",
    "        for _ in range(num_layers):\n",
    "            layers.extend([nn.Linear(num_neurons, num_neurons), act])\n",
    "\n",
    "        # output layer\n",
    "        layers.append(nn.Linear(num_neurons, 1))\n",
    "\n",
    "        # build the network\n",
    "        self.network = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return self.network(x.reshape(-1, 1)).squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aad9e67c-4bca-466e-96e5-c2f3cb689fa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.func import functional_call, grad, vmap\n",
    "\n",
    "model = LinearNN()\n",
    "\n",
    "# notice that `functional_call` supports batched inputs by default\n",
    "# thus there is not need to call vmap on it, as it's instead the case\n",
    "# for the derivative calls\n",
    "def f(x: torch.Tensor, params: dict[str, torch.nn.Parameter]) -> torch.Tensor:\n",
    "    return functional_call(model, params_dict, (x, ))\n",
    "\n",
    "# return function for computing higher order gradients with respect\n",
    "# to input by simply composing `grad` calls and use again `vmap` for\n",
    "# efficient batching of the input\n",
    "dfdx = vmap(grad(f), in_dims=(0, None))\n",
    "d2fdx2 = vmap(grad(grad(f)), in_dims=(0, None))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8751f8b9-95c8-4e11-b20f-99387a394742",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "R = 1.0  # rate of maximum population growth parameterizing the equation\n",
    "X_BOUNDARY = 0.0  # boundary condition coordinate\n",
    "F_BOUNDARY = 0.5  # boundary condition value\n",
    "\n",
    "def loss_fn(params: torch.Tensor, x: torch.Tensor):\n",
    "\n",
    "    # interior loss\n",
    "    f_value = f(x, params)\n",
    "    interior = dfdx(x, params) - R * f_value * (1 - f_value)\n",
    "\n",
    "    # boundary loss\n",
    "    x0 = X_BOUNDARY\n",
    "    f0 = F_BOUNDARY\n",
    "    x_boundary = torch.tensor([x0])\n",
    "    f_boundary = torch.tensor([f0])\n",
    "    boundary = f(x_boundary, params) - f_boundary\n",
    "\n",
    "    loss = nn.MSELoss()\n",
    "    loss_value = loss(interior, torch.zeros_like(interior)) + loss(\n",
    "        boundary, torch.zeros_like(boundary)\n",
    "    )\n",
    "\n",
    "    return loss_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b91552b-1e85-4c27-9e53-babbabe75f62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# choose the configuration for the training loop\n",
    "batch_size = 30  # number of colocation points to sample in the domain\n",
    "num_iter = 100  # maximum number of iterations\n",
    "learning_rate = 1e-1  # learning rate\n",
    "domain = (-5.0, 5.0)  # ;ogistic equation domain\n",
    "\n",
    "# choose optimizer with functional API using functorch\n",
    "optimizer = torchopt.FuncOptimizer(torchopt.adam(lr=learning_rate))\n",
    "\n",
    "# initial parameters randomly initialized\n",
    "params = tuple(model.parameters())\n",
    "\n",
    "# train the model\n",
    "for i in range(num_iter):\n",
    "\n",
    "    # sample points in the domain randomly for each epoch\n",
    "    x = torch.FloatTensor(batch_size).uniform_(domain[0], domain[1])\n",
    "\n",
    "    # compute the loss with the current parameters\n",
    "    loss = loss_fn(params, x)\n",
    "\n",
    "    # update the parameters with functional optimizer\n",
    "    params = optimizer.step(loss, params)\n",
    "\n",
    "    print(f\"Iteration {i} with loss {float(loss)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
