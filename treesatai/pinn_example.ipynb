{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2e74685f-58ca-4865-8f90-241cb459f2f6",
   "metadata": {},
   "source": [
    "### source\n",
    "\n",
    "https://towardsdatascience.com/solving-differential-equations-with-neural-networks-afdcf7b8bcc4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0b6920dc-e88e-4260-b2ae-4f6ab48eeb27",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchopt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e4522011-24db-4a72-bf1e-ddc5b2942dbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearNN(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_inputs: int = 1,\n",
    "        num_layers: int = 1,\n",
    "        num_neurons: int = 5,\n",
    "        act: nn.Module = nn.Tanh(),\n",
    "    ) -> None:\n",
    "        \"\"\"Basic neural network architecture with linear layers\n",
    "        \n",
    "        Args:\n",
    "            num_inputs (int, optional): the dimensionality of the input tensor\n",
    "            num_layers (int, optional): the number of hidden layers\n",
    "            num_neurons (int, optional): the number of neurons for each hidden layer\n",
    "            act (nn.Module, optional): the non-linear activation function to use for stitching\n",
    "                linear layers togeter\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        \n",
    "        self.num_inputs = num_inputs\n",
    "        self.num_neurons = num_neurons\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        layers = []\n",
    "\n",
    "        # input layer\n",
    "        layers.append(nn.Linear(self.num_inputs, num_neurons))\n",
    "\n",
    "        # hidden layers with linear layer and activation\n",
    "        for _ in range(num_layers):\n",
    "            layers.extend([nn.Linear(num_neurons, num_neurons), act])\n",
    "\n",
    "        # output layer\n",
    "        layers.append(nn.Linear(num_neurons, 1))\n",
    "\n",
    "        # build the network\n",
    "        self.network = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return self.network(x.reshape(-1, 1)).squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "aad9e67c-4bca-466e-96e5-c2f3cb689fa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.func import functional_call, grad, vmap\n",
    "\n",
    "model = LinearNN()\n",
    "\n",
    "# notice that `functional_call` supports batched inputs by default\n",
    "# thus there is not need to call vmap on it, as it's instead the case\n",
    "# for the derivative calls\n",
    "def f(x: torch.Tensor, params: dict[str, torch.nn.Parameter]) -> torch.Tensor:\n",
    "    return functional_call(model, params_dict, (x, ))\n",
    "\n",
    "# return function for computing higher order gradients with respect\n",
    "# to input by simply composing `grad` calls and use again `vmap` for\n",
    "# efficient batching of the input\n",
    "dfdx = vmap(grad(f), in_dims=(0, None))\n",
    "d2fdx2 = vmap(grad(grad(f)), in_dims=(0, None))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8751f8b9-95c8-4e11-b20f-99387a394742",
   "metadata": {},
   "outputs": [],
   "source": [
    "R = 1.0  # rate of maximum population growth parameterizing the equation\n",
    "X_BOUNDARY = 0.0  # boundary condition coordinate\n",
    "F_BOUNDARY = 0.5  # boundary condition value\n",
    "\n",
    "def loss_fn(params: torch.Tensor, x: torch.Tensor):\n",
    "\n",
    "    # interior loss\n",
    "    f_value = f(x, params)\n",
    "    interior = dfdx(x, params) - R * f_value * (1 - f_value)\n",
    "\n",
    "    # boundary loss\n",
    "    x0 = X_BOUNDARY\n",
    "    f0 = F_BOUNDARY\n",
    "    x_boundary = torch.tensor([x0])\n",
    "    f_boundary = torch.tensor([f0])\n",
    "    boundary = f(x_boundary, params) - f_boundary\n",
    "\n",
    "    loss = nn.MSELoss()\n",
    "    loss_value = loss(interior, torch.zeros_like(interior)) + loss(\n",
    "        boundary, torch.zeros_like(boundary)\n",
    "    )\n",
    "\n",
    "    return loss_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8b91552b-1e85-4c27-9e53-babbabe75f62",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'params_dict' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 20\u001b[0m\n\u001b[1;32m     17\u001b[0m x \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mFloatTensor(batch_size)\u001b[38;5;241m.\u001b[39muniform_(domain[\u001b[38;5;241m0\u001b[39m], domain[\u001b[38;5;241m1\u001b[39m])\n\u001b[1;32m     19\u001b[0m \u001b[38;5;66;03m# compute the loss with the current parameters\u001b[39;00m\n\u001b[0;32m---> 20\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[43mloss_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;66;03m# update the parameters with functional optimizer\u001b[39;00m\n\u001b[1;32m     23\u001b[0m params \u001b[38;5;241m=\u001b[39m optimizer\u001b[38;5;241m.\u001b[39mstep(loss, params)\n",
      "Cell \u001b[0;32mIn[15], line 8\u001b[0m, in \u001b[0;36mloss_fn\u001b[0;34m(params, x)\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mloss_fn\u001b[39m(params: torch\u001b[38;5;241m.\u001b[39mTensor, x: torch\u001b[38;5;241m.\u001b[39mTensor):\n\u001b[1;32m      6\u001b[0m \n\u001b[1;32m      7\u001b[0m     \u001b[38;5;66;03m# interior loss\u001b[39;00m\n\u001b[0;32m----> 8\u001b[0m     f_value \u001b[38;5;241m=\u001b[39m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      9\u001b[0m     interior \u001b[38;5;241m=\u001b[39m dfdx(x, params) \u001b[38;5;241m-\u001b[39m R \u001b[38;5;241m*\u001b[39m f_value \u001b[38;5;241m*\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m f_value)\n\u001b[1;32m     11\u001b[0m     \u001b[38;5;66;03m# boundary loss\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[14], line 9\u001b[0m, in \u001b[0;36mf\u001b[0;34m(x, params)\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mf\u001b[39m(x: torch\u001b[38;5;241m.\u001b[39mTensor, params: \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mParameter]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m torch\u001b[38;5;241m.\u001b[39mTensor:\n\u001b[0;32m----> 9\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m functional_call(model, \u001b[43mparams_dict\u001b[49m, (x, ))\n",
      "\u001b[0;31mNameError\u001b[0m: name 'params_dict' is not defined"
     ]
    }
   ],
   "source": [
    "# choose the configuration for the training loop\n",
    "batch_size = 30  # number of colocation points to sample in the domain\n",
    "num_iter = 100  # maximum number of iterations\n",
    "learning_rate = 1e-1  # learning rate\n",
    "domain = (-5.0, 5.0)  # ;ogistic equation domain\n",
    "\n",
    "# choose optimizer with functional API using functorch\n",
    "optimizer = torchopt.FuncOptimizer(torchopt.adam(lr=learning_rate))\n",
    "\n",
    "# initial parameters randomly initialized\n",
    "params = tuple(model.parameters())\n",
    "\n",
    "# train the model\n",
    "for i in range(num_iter):\n",
    "\n",
    "    # sample points in the domain randomly for each epoch\n",
    "    x = torch.FloatTensor(batch_size).uniform_(domain[0], domain[1])\n",
    "\n",
    "    # compute the loss with the current parameters\n",
    "    loss = loss_fn(params, x)\n",
    "\n",
    "    # update the parameters with functional optimizer\n",
    "    params = optimizer.step(loss, params)\n",
    "\n",
    "    print(f\"Iteration {i} with loss {float(loss)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6dec862-b66f-446a-81d8-d6b6a31c4447",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d4fc71f-1e27-4b26-95ae-9147765ff947",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
