{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "45a09dcb-b90b-45d7-b5a2-54996a34ada0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-03 16:00:04.442154: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-05-03 16:00:05.314637: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "import datetime\n",
    "from pathlib import Path\n",
    "\n",
    "import intake\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from tensorflow.keras.regularizers import l2\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.layers import Input, Conv3D, Activation, BatchNormalization, \\\n",
    "                                     Dropout, Flatten, Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "17759222-56d5-4a21-ba7d-c508b05ea21c",
   "metadata": {},
   "outputs": [],
   "source": [
    "catalog = intake.open_catalog(Path('../catalog.yml'))\n",
    "source = getattr(catalog, 'treesat_multi')\n",
    "df = source.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0749fbd3-b6ee-443c-b12c-741c2ee1f1bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_bands = [f'B{x}' for x in range(2, 9)] + ['B8A', 'B11', 'B12', 'TCI_R', 'TCI_G', 'TCI_B']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d5926fbe-15c2-4fa6-8977-99eafdcb5ca3",
   "metadata": {},
   "outputs": [],
   "source": [
    "season = 3\n",
    "target = source.metadata['categories']['multi'] # multi / trinary\n",
    "# labels = np.ceil(df[target].to_numpy())\n",
    "# labels = (df[target].to_numpy() > 0.3).astype(float)\n",
    "labels = df[target].to_numpy()\n",
    "all_data = []\n",
    "\n",
    "filepaths = sorted(list(Path('seasonal_median').glob(f'processed*{season}.npy')))\n",
    "for filepath in filepaths:\n",
    "    with open(filepath, 'rb') as f:\n",
    "        data = np.load(f)\n",
    "    all_data.append(data)\n",
    "\n",
    "features = np.stack(all_data, axis=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7a5fb97e-b021-4620-9b31-468323822101",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Conv3DModelCreator:\n",
    "    def normalise(self, a, p=1):\n",
    "        upper = np.percentile(a, 100-p)\n",
    "        lower = np.percentile(a, p)\n",
    "    \n",
    "        bounded_a = np.where(a > upper, np.median(a), a)\n",
    "        bounded_a = np.where(a < lower, np.median(a), bounded_a)\n",
    "        \n",
    "        scaled_a = (bounded_a - lower)/(upper - lower)\n",
    "        return scaled_a\n",
    "        \n",
    "    def split_and_normalise(self, y, X, random_state):\n",
    "        \"\"\"Split and max scale.\"\"\"\n",
    "        X_train, X_test, y_train, y_test = train_test_split(\n",
    "            X, y, test_size=0.1, random_state=random_state)\n",
    "\n",
    "        for i in range(X_train.shape[-1]):\n",
    "            X_train[...,i] = self.normalise(X_train[...,i])\n",
    "            X_test[...,i] = self.normalise(X_test[...,i])\n",
    "        \n",
    "        return X_train, X_test, y_train, y_test\n",
    "\n",
    "    def f1_score(self, y_true, y_logit):\n",
    "        true_positives = K.sum(K.round(K.clip(y_true * y_logit, 0, 1)))\n",
    "        possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "        \n",
    "        recall = true_positives / (possible_positives + K.epsilon())\n",
    "        predicted_positives = K.sum(K.round(K.clip(y_logit, 0, 1)))\n",
    "        precision = true_positives / (predicted_positives + K.epsilon())\n",
    "        return (2 * precision * recall) / (precision + recall + K.epsilon())\n",
    "\n",
    "    def build_model_3d(self, y_train, input_size, metrics):\n",
    "        m = tf.keras.Sequential()\n",
    "        m.add(Input(input_size))\n",
    "        for i in range(2): \n",
    "            m.add(Conv3D(\n",
    "                64, \n",
    "                strides=1, \n",
    "                padding='same',\n",
    "                kernel_size=(3, 3, 3),\n",
    "                kernel_initializer='he_normal',\n",
    "                kernel_regularizer=l2(1e-6)\n",
    "            ))\n",
    "            m.add(BatchNormalization(axis=-1))\n",
    "            m.add(Activation('relu'))\n",
    "            m.add(Dropout(0.25))\n",
    "            \n",
    "        m.add(Flatten())\n",
    "        m.add(Dense(128, \n",
    "                    kernel_initializer='he_normal', \n",
    "                    kernel_regularizer=l2(1e-6)\n",
    "                   )\n",
    "             )\n",
    "        m.add(BatchNormalization(axis=-1))\n",
    "        m.add(Activation('relu'))\n",
    "        m.add(Dropout(0.25))\n",
    "        \n",
    "        m.add(Dense(\n",
    "            y_train.shape[1], \n",
    "            activation='softmax', \n",
    "            kernel_initializer='glorot_uniform',\n",
    "            kernel_regularizer=l2(1e-6)\n",
    "        ))\n",
    "\n",
    "        # print(m.summary())\n",
    "\n",
    "        opt = tf.keras.optimizers.Adam(\n",
    "            learning_rate=0.001,\n",
    "            beta_1=0.9,\n",
    "            beta_2=0.999,\n",
    "            epsilon=1e-07\n",
    "        )\n",
    "        m.compile(\n",
    "            optimizer=opt,\n",
    "            # loss=tf.keras.losses.MeanSquaredError(),\n",
    "            loss=tf.keras.losses.MeanAbsoluteError(),\n",
    "            # loss=tf.keras.losses.CategoricalCrossentropy(),\n",
    "            metrics=metrics\n",
    "        )\n",
    "        return m\n",
    "        \n",
    "    def run(self, y, X, model_name, epochs, overwrite=False):\n",
    "        model_dir = Path('models')\n",
    "        \n",
    "        model_path = model_dir.joinpath(model_name)\n",
    "        model_savepoint = model_dir.joinpath(model_path.stem)\n",
    "        log_file = model_path.with_suffix('.log')\n",
    "\n",
    "        if overwrite:\n",
    "            for f in [model_path, log_file] + list(model_savepoint.glob('*')):\n",
    "                f.unlink(missing_ok=True)\n",
    "\n",
    "        X_train, X_test, y_train, y_test = self.split_and_normalise(y, X, random_state=42)\n",
    "        \n",
    "        builtin_metrics = ['accuracy', 'binary_accuracy', 'mean_squared_error', 'mean_absolute_error']\n",
    "        custom_metrics = [self.f1_score]\n",
    "        \n",
    "        if model_path.is_file():\n",
    "            model = tf.keras.models.load_model(model_path)\n",
    "        else:\n",
    "            model = self.build_model_3d(y_train, X_train.shape[1:], builtin_metrics + custom_metrics)\n",
    "\n",
    "        callbacks = [\n",
    "            tf.keras.callbacks.BackupAndRestore(\n",
    "                model_savepoint, save_freq='epoch', delete_checkpoint=False\n",
    "            ),\n",
    "            tf.keras.callbacks.CSVLogger(\n",
    "                log_file, append=True\n",
    "            )\n",
    "        ]\n",
    "\n",
    "        if log_file.is_file():\n",
    "            df = pd.read_csv(log_file)[['epoch', 'loss'] + builtin_metrics + ['f1_score']]\n",
    "            df['epoch'] += 1\n",
    "            print('Previous training:')\n",
    "            display(HTML(df.to_html(index=False)))\n",
    "        \n",
    "        model.fit(\n",
    "            X_train, y_train, epochs=epochs, verbose=1, batch_size=4, callbacks=callbacks,\n",
    "            validation_data=(X_test, y_test))\n",
    "\n",
    "        model.save(model_path)\n",
    "            \n",
    "        # preds = model.predict(X_test, verbose=0)\n",
    "        return model.evaluate(X_test, y_test, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e930bc4-4c27-4992-b2c3-6f0180070568",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "\u001b[1m11336/11336\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m359s\u001b[0m 32ms/step - accuracy: 0.4343 - binary_accuracy: 0.8666 - f1_score: 0.4139 - loss: 0.0785 - mean_absolute_error: 0.0758 - mean_squared_error: 0.0531 - val_accuracy: 0.5289 - val_binary_accuracy: 0.8716 - val_f1_score: 0.5188 - val_loss: 0.0709 - val_mean_absolute_error: 0.0642 - val_mean_squared_error: 0.0473\n",
      "Epoch 2/100\n",
      "\u001b[1m11336/11336\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m356s\u001b[0m 31ms/step - accuracy: 0.5103 - binary_accuracy: 0.8708 - f1_score: 0.4979 - loss: 0.0735 - mean_absolute_error: 0.0663 - mean_squared_error: 0.0487 - val_accuracy: 0.5813 - val_binary_accuracy: 0.8763 - val_f1_score: 0.5722 - val_loss: 0.0661 - val_mean_absolute_error: 0.0576 - val_mean_squared_error: 0.0428\n",
      "Epoch 3/100\n",
      "\u001b[1m11336/11336\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m354s\u001b[0m 31ms/step - accuracy: 0.5453 - binary_accuracy: 0.8733 - f1_score: 0.5345 - loss: 0.0705 - mean_absolute_error: 0.0620 - mean_squared_error: 0.0455 - val_accuracy: 0.5797 - val_binary_accuracy: 0.8766 - val_f1_score: 0.5695 - val_loss: 0.0664 - val_mean_absolute_error: 0.0580 - val_mean_squared_error: 0.0418\n",
      "Epoch 4/100\n",
      "\u001b[1m11336/11336\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m353s\u001b[0m 31ms/step - accuracy: 0.5526 - binary_accuracy: 0.8739 - f1_score: 0.5418 - loss: 0.0697 - mean_absolute_error: 0.0613 - mean_squared_error: 0.0443 - val_accuracy: 0.6031 - val_binary_accuracy: 0.8784 - val_f1_score: 0.5915 - val_loss: 0.0640 - val_mean_absolute_error: 0.0554 - val_mean_squared_error: 0.0408\n",
      "Epoch 5/100\n",
      "\u001b[1m11336/11336\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m353s\u001b[0m 31ms/step - accuracy: 0.5649 - binary_accuracy: 0.8751 - f1_score: 0.5523 - loss: 0.0683 - mean_absolute_error: 0.0599 - mean_squared_error: 0.0431 - val_accuracy: 0.5727 - val_binary_accuracy: 0.8757 - val_f1_score: 0.5602 - val_loss: 0.0674 - val_mean_absolute_error: 0.0589 - val_mean_squared_error: 0.0441\n",
      "Epoch 6/100\n",
      "\u001b[1m11336/11336\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m354s\u001b[0m 31ms/step - accuracy: 0.5706 - binary_accuracy: 0.8748 - f1_score: 0.5588 - loss: 0.0677 - mean_absolute_error: 0.0593 - mean_squared_error: 0.0427 - val_accuracy: 0.6216 - val_binary_accuracy: 0.8793 - val_f1_score: 0.6123 - val_loss: 0.0612 - val_mean_absolute_error: 0.0530 - val_mean_squared_error: 0.0380\n",
      "Epoch 7/100\n",
      "\u001b[1m11336/11336\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m353s\u001b[0m 31ms/step - accuracy: 0.5716 - binary_accuracy: 0.8755 - f1_score: 0.5611 - loss: 0.0671 - mean_absolute_error: 0.0588 - mean_squared_error: 0.0420 - val_accuracy: 0.6065 - val_binary_accuracy: 0.8786 - val_f1_score: 0.5920 - val_loss: 0.0633 - val_mean_absolute_error: 0.0550 - val_mean_squared_error: 0.0401\n",
      "Epoch 8/100\n",
      "\u001b[1m11336/11336\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m349s\u001b[0m 31ms/step - accuracy: 0.5775 - binary_accuracy: 0.8762 - f1_score: 0.5670 - loss: 0.0665 - mean_absolute_error: 0.0582 - mean_squared_error: 0.0415 - val_accuracy: 0.6295 - val_binary_accuracy: 0.8784 - val_f1_score: 0.6164 - val_loss: 0.0613 - val_mean_absolute_error: 0.0527 - val_mean_squared_error: 0.0377\n",
      "Epoch 9/100\n",
      "\u001b[1m11336/11336\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m348s\u001b[0m 31ms/step - accuracy: 0.5844 - binary_accuracy: 0.8768 - f1_score: 0.5726 - loss: 0.0659 - mean_absolute_error: 0.0574 - mean_squared_error: 0.0411 - val_accuracy: 0.6386 - val_binary_accuracy: 0.8805 - val_f1_score: 0.6301 - val_loss: 0.0597 - val_mean_absolute_error: 0.0511 - val_mean_squared_error: 0.0362\n",
      "Epoch 10/100\n",
      "\u001b[1m11336/11336\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m348s\u001b[0m 31ms/step - accuracy: 0.5874 - binary_accuracy: 0.8764 - f1_score: 0.5761 - loss: 0.0657 - mean_absolute_error: 0.0572 - mean_squared_error: 0.0408 - val_accuracy: 0.6313 - val_binary_accuracy: 0.8801 - val_f1_score: 0.6179 - val_loss: 0.0604 - val_mean_absolute_error: 0.0521 - val_mean_squared_error: 0.0371\n",
      "Epoch 11/100\n",
      "\u001b[1m 8373/11336\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m1:29\u001b[0m 30ms/step - accuracy: 0.5863 - binary_accuracy: 0.8762 - f1_score: 0.5735 - loss: 0.0656 - mean_absolute_error: 0.0572 - mean_squared_error: 0.0407"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "model_name = f'conv_all_{season}_multi_multi.keras'\n",
    "# If not overwrite and there's an existing model, the model will \n",
    "# continue training if the given epoch is bigger than the previous,\n",
    "# else just evaluate.\n",
    "# Ensure train splits are the same across continuations / evaluations\n",
    "# by not modifying the random_state in split_and_normalise.\n",
    "Conv3DModelCreator().run(\n",
    "    labels, features, model_name, epochs=100, overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "935ea723-4532-41a7-81c7-ac5041830ebf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad7f6adf-46ce-44d5-afc2-7874169f2925",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42ddfd5d-fa5f-4a62-a7bd-eeaa21182498",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c949c04-c3d4-4444-b150-eeba8e633247",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f83d290-44bc-4e0a-bf92-ecfc0a03825a",
   "metadata": {},
   "outputs": [],
   "source": [
    "positive_weights = labels.shape[0] / (np.count_nonzero(labels, axis=0) * labels.shape[1])\n",
    "negative_weights = labels.shape[0] / (np.count_nonzero(labels==0, axis=0) * labels.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "405b74d7-5f3a-43ba-bf6f-61cb4c608694",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55f8a5ee-1b88-4023-aab6-b17436811e83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cm = tf.math.confusion_matrix(labels, predictions, num_classes=None)\n",
    "# px.imshow(\n",
    "#     np.array(cm),\n",
    "#     # animation_frame=0,\n",
    "#     labels=dict(color=\"Corr coef\"),\n",
    "#     x=cm[0].index,\n",
    "#     y=cm[0].columns,\n",
    "#     title='Confusion Matrix',\n",
    "#     text_auto=True, aspect='auto', zmin=0, height=500\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeb40e4b-059f-4dda-b602-d603e1c22f18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# target = source.metadata['categories']['multi'] # multi / trinary\n",
    "# labels = df[target].astype('category').cat.codes\n",
    "# seasons = ['Spring', 'Summer', 'Autumn', 'Winter']\n",
    "# all_data = []\n",
    "# for season in seasons:\n",
    "#     filepath = sorted(list(Path('seasonal_median').glob(f'{season}.npy')))\n",
    "#     with open(filepath[0], 'rb') as f:\n",
    "#         all_data.append(np.load(f))\n",
    "    \n",
    "# model_name = f'conv_all_mean_seasons_multi.keras'\n",
    "\n",
    "# features = np.stack(all_data, axis=3)\n",
    "# score = ConvModelCreator().run(labels, features, model_name)\n",
    "# score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f4c44e9-a2d6-4cb7-ba97-869f17f9733a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# traces = [go.Scatter(x=seasons, y=seasonal_scores)]\n",
    "# go.Figure(\n",
    "#     data=traces,\n",
    "#     layout={\n",
    "#         \"xaxis\": {\"title\": \"Season\"},\n",
    "#         \"yaxis\": {\"title\": \"Accuracy\"},\n",
    "#         \"title\": \"Conv2d accuracies\"}\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "343067ff-97a9-46a6-8d85-b553a792dbaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# target = source.metadata['categories']['trinary'] # multi / trinary\n",
    "# labels = df[target].astype('category').cat.codes\n",
    "# seasons = ['Spring', 'Summer', 'Autumn', 'Winter']\n",
    "# all_data = []\n",
    "# for season in tqdm(seasons):\n",
    "#     filepath = sorted(list(Path('seasonal_median').glob(f'{season}.npy')))\n",
    "#     with open(filepath[0], 'rb') as f:\n",
    "#         all_data.append(np.load(f))\n",
    "    \n",
    "# model_name = f'conv_all_mean_seasons_trinary.keras'\n",
    "# score = ConvModelCreator().run(labels, np.stack(all_data, axis=3), model_name)\n",
    "# score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49338456-9905-4cef-bf76-db246dc84d39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# target = source.metadata['categories']['trinary'] # multi / trinary\n",
    "\n",
    "# mask = df[target] != 'cleared'\n",
    "\n",
    "# labels = df[target][mask].astype('category').cat.codes\n",
    "\n",
    "# seasons = ['Spring', 'Summer', 'Autumn', 'Winter']\n",
    "# all_data = []\n",
    "# for season in tqdm(seasons):\n",
    "#     filepath = sorted(list(Path('seasonal_median').glob(f'{season}.npy')))\n",
    "#     with open(filepath[0], 'rb') as f:\n",
    "#         data = np.load(f)\n",
    "#         all_data.append(data[mask])\n",
    "        \n",
    "# model_name = f'conv_all_mean_seasons_binary.keras'\n",
    "# score = ConvModelCreator().run(labels, np.stack(all_data, axis=3), model_name)\n",
    "# score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab0f8aaa-cc12-44e2-a887-9d8af6b5fef6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40222121-e554-4334-8698-f86951553aa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# filepaths = sorted(list(Path('london').glob('*.npy')), key=lambda x: int(x.stem.split('_')[-1]))\n",
    "\n",
    "# n_chunks = 50000\n",
    "# chunks = [gdf[i: i + n_chunks] for i in range(0, gdf.shape[0], n_chunks)]\n",
    "# scores = []\n",
    "\n",
    "# chunk[target].cat.codes\n",
    "\n",
    "# for chunk, filepath in tqdm(zip(chunks, filepaths), total=len(filepaths)\n",
    "#                            ):\n",
    "#     score = LightModelCreator().run_and_eval(chunk[target].cat.codes, [filepath])\n",
    "#     scores.append(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82157b0b-aa81-48ee-a15a-6a533370c3f8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1156bde7-a279-465b-9c62-a9c34ae0f829",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f4a1dee-1b34-4cbd-8037-37702377a9be",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1a88adb-eca0-44ba-9b83-49adb7994137",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2dfa99c-b32b-4bb5-8994-2b54acbe439f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49ee6c92-f989-4253-b7de-08a50c4e40cb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5189141-5f07-4aac-a68f-00a24ec5d69d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3378d43-42ae-4f34-9df2-6e74e58f1822",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
