{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "45a09dcb-b90b-45d7-b5a2-54996a34ada0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-17 23:23:20.283502: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-05-17 23:23:21.426282: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "import datetime\n",
    "from pathlib import Path\n",
    "\n",
    "import intake\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "from tensorflow.keras.regularizers import l2\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import backend as K\n",
    "\n",
    "from tensorflow.keras.layers import ConvLSTM2D, Conv2D, Conv3D, Dense, TimeDistributed, MaxPooling2D, GlobalAveragePooling2D \n",
    "from tensorflow.keras.layers import Activation, Dropout, Flatten, BatchNormalization, Input, LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "17759222-56d5-4a21-ba7d-c508b05ea21c",
   "metadata": {},
   "outputs": [],
   "source": [
    "catalog = intake.open_catalog(Path('../catalog.yml'))\n",
    "source = getattr(catalog, 'treesat_multi')\n",
    "df = source.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0749fbd3-b6ee-443c-b12c-741c2ee1f1bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_bands = [f'B{x}' for x in range(2, 9)] + ['B8A', 'B11', 'B12', 'TCI_R', 'TCI_G', 'TCI_B']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d5926fbe-15c2-4fa6-8977-99eafdcb5ca3",
   "metadata": {},
   "outputs": [],
   "source": [
    "target = source.metadata['categories']['multi'] # multi / trinary\n",
    "\n",
    "labels = df[target].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7a5fb97e-b021-4620-9b31-468323822101",
   "metadata": {},
   "outputs": [],
   "source": [
    "class KerasModelCreator:\n",
    "    def crop_y(self, y):\n",
    "        lower = 0.1\n",
    "        bounded_y = np.where(y < lower, 0.0, y)\n",
    "        rescaled_y = bounded_y/bounded_y.sum(axis=1, keepdims=1)\n",
    "        return rescaled_y\n",
    "        \n",
    "    def normalise_X(self, X, p=1):\n",
    "        upper = np.percentile(X, 100-p)\n",
    "        lower = np.percentile(X, p)\n",
    "    \n",
    "        bounded_X = np.where(X > upper, np.median(X), X)\n",
    "        bounded_X = np.where(X < lower, np.median(X), bounded_X)\n",
    "        \n",
    "        scaled_X = (bounded_X - lower)/(upper - lower)\n",
    "        return scaled_X\n",
    "        \n",
    "    def split_and_normalise(self, y, X, random_state=42, test_size=0.1):\n",
    "        \"\"\"Split and max scale.\"\"\"\n",
    "        X_train, X_test, y_train, y_test = train_test_split(\n",
    "            X, y, test_size=test_size, random_state=random_state, shuffle=False)\n",
    "\n",
    "        y_train, y_test = self.crop_y(y_train), self.crop_y(y_test)\n",
    "\n",
    "        for i in range(X_train.shape[-1]):\n",
    "            X_train[...,i] = self.normalise_X(X_train[...,i])\n",
    "            X_test[...,i] = self.normalise_X(X_test[...,i])\n",
    "        \n",
    "        return X_train, X_test, y_train, y_test\n",
    "\n",
    "    def build_model(self, y_train, input_size, metrics):\n",
    "        filters_scale = 128\n",
    "        dropout_base = 0.25\n",
    "        \n",
    "        m = tf.keras.Sequential()\n",
    "\n",
    "        m.add(Input(input_size))\n",
    "\n",
    "        m.add(Conv2D(\n",
    "            filters=filters_scale*2, kernel_size=5, \n",
    "            padding='same', activation='relu'\n",
    "        ))\n",
    "        m.add(MaxPooling2D())\n",
    "\n",
    "        m.add(Conv2D(\n",
    "            filters=filters_scale, kernel_size=5, \n",
    "            padding='same', activation='relu'\n",
    "        ))\n",
    "        m.add(MaxPooling2D())\n",
    "\n",
    "        m.add(GlobalAveragePooling2D())\n",
    "\n",
    "        fm = tf.keras.Sequential()\n",
    "        fm.add(Input((None, *input_size)))\n",
    "        fm.add(TimeDistributed(m))\n",
    "\n",
    "        fm.add(LSTM(filters_scale))\n",
    "\n",
    "        fm.add(Dropout(dropout_base*2))\n",
    "        \n",
    "        fm.add(Dense(filters_scale*2, activation=\"relu\"))\n",
    "        fm.add(Dropout(dropout_base))\n",
    "\n",
    "        fm.add(Dense(filters_scale, activation=\"relu\"))\n",
    "        fm.add(Dropout(dropout_base))\n",
    "        \n",
    "        fm.add(Dense(\n",
    "            y_train.shape[1], \n",
    "            activation='softmax', \n",
    "        ))\n",
    "\n",
    "        loss = tf.keras.losses.Huber(\n",
    "            delta=1.0,\n",
    "            reduction='sum_over_batch_size'\n",
    "        )\n",
    "\n",
    "        opt = tf.keras.optimizers.Adam(\n",
    "            learning_rate=0.001\n",
    "        )\n",
    "        fm.compile(\n",
    "            optimizer=opt,\n",
    "            loss=loss,\n",
    "            metrics=metrics\n",
    "        )\n",
    "        return fm\n",
    "\n",
    "        \n",
    "    def run(self, y, X, model_path, epochs=10, overwrite=False):\n",
    "        ''' \n",
    "        If not overwrite and there's an existing model, the model will \n",
    "        continue training if the given epoch is bigger than the previous,\n",
    "        else just evaluate.\n",
    "        Ensure train splits are the same across continuations / evaluations\n",
    "        by not modifying the random_state in split_and_normalise.\n",
    "        '''\n",
    "        model_savepoint = model_path.parent.joinpath(model_path.stem)\n",
    "        log_file = model_path.with_suffix('.log')\n",
    "\n",
    "        if overwrite:\n",
    "            for f in [model_path, log_file] + list(model_savepoint.glob('*')):\n",
    "                f.unlink(missing_ok=True)\n",
    "\n",
    "        X_train, X_test, y_train, y_test = self.split_and_normalise(y, X, random_state=42)\n",
    "        \n",
    "        default_metrics = ['accuracy', 'root_mean_squared_error', 'mean_squared_error', 'r2_score']\n",
    "        \n",
    "        if model_path.is_file():\n",
    "            model = tf.keras.models.load_model(model_path)\n",
    "        else:\n",
    "            model = self.build_model(y_train, X_train.shape, default_metrics)\n",
    "\n",
    "        callbacks = [\n",
    "            tf.keras.callbacks.BackupAndRestore(\n",
    "                model_savepoint, save_freq='epoch', delete_checkpoint=False\n",
    "            ),\n",
    "            tf.keras.callbacks.CSVLogger(\n",
    "                log_file, append=True\n",
    "            )\n",
    "        ]\n",
    "\n",
    "        if log_file.is_file():\n",
    "            df = pd.read_csv(log_file)[['epoch', 'loss'] + default_metrics]\n",
    "            df['epoch'] += 1\n",
    "            print('Previous training:')\n",
    "                \n",
    "            display(HTML(df.to_html(index=False)))\n",
    "        \n",
    "        model.fit(\n",
    "            X_train, y_train, epochs=epochs, verbose=1, batch_size=1, callbacks=callbacks, shuffle=False\n",
    "        )\n",
    "\n",
    "        model.save(model_path)\n",
    "        \n",
    "        return model.evaluate(X_test, y_test, verbose=0, return_dict=True)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e930bc4-4c27-4992-b2c3-6f0180070568",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dir = Path('models').joinpath('seasons')\n",
    "model_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "shuffle_seed = 42\n",
    "shuffled_labels = shuffle(labels, random_state=shuffle_seed)\n",
    "\n",
    "test_years = [2020]\n",
    "\n",
    "train_years = [2017, 2018, 2019]\n",
    "all_seasons = [3, 6, 9, 12]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ace0a102-03f4-4825-8a3e-43dae2a2c169",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "405b74d7-5f3a-43ba-bf6f-61cb4c608694",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab0f8aaa-cc12-44e2-a887-9d8af6b5fef6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82157b0b-aa81-48ee-a15a-6a533370c3f8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1156bde7-a279-465b-9c62-a9c34ae0f829",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f4a1dee-1b34-4cbd-8037-37702377a9be",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1a88adb-eca0-44ba-9b83-49adb7994137",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2dfa99c-b32b-4bb5-8994-2b54acbe439f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49ee6c92-f989-4253-b7de-08a50c4e40cb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5189141-5f07-4aac-a68f-00a24ec5d69d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3378d43-42ae-4f34-9df2-6e74e58f1822",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tf.keras.utils.get_custom_objects().clear()\n",
    "# @tf.keras.utils.register_keras_serializable(name='f1_majority')\n",
    "# def f1_majority(y_true, y_pred):\n",
    "#     # Convert multilabel proportions to single-label majority class\n",
    "#     majority_class = (y_true == tf.keras.ops.amax(y_true, keepdims=True, axis=1))\n",
    "#     majority_class = tf.cast(majority_class, tf.float32)\n",
    "    \n",
    "#     true_positives = K.sum(K.round(majority_class * y_pred))\n",
    "#     possible_positives = K.sum(K.round(majority_class))\n",
    "\n",
    "#     recall = true_positives / (possible_positives + K.epsilon())\n",
    "    \n",
    "#     predicted_positives = K.sum(K.round(y_pred))\n",
    "\n",
    "#     precision = true_positives / (predicted_positives + K.epsilon())\n",
    "    \n",
    "#     return 2 * precision * recall / (precision + recall + K.epsilon())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
