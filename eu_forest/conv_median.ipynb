{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "45a09dcb-b90b-45d7-b5a2-54996a34ada0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-23 13:32:41.514807: I external/local_tsl/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-06-23 13:32:41.606598: I external/local_tsl/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-06-23 13:32:41.979041: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-06-23 13:32:43.536544: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "from IPython.display import display, HTML\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "from time import sleep\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix, ConfusionMatrixDisplay, multilabel_confusion_matrix\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.layers import *\n",
    "from tensorflow.keras.optimizers import Adam, RMSprop, Nadam, SGD, AdamW\n",
    "from tensorflow.keras.models import load_model, Model\n",
    "from tensorflow.keras.initializers import glorot_uniform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0749fbd3-b6ee-443c-b12c-741c2ee1f1bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_bands = [f'B{x}' for x in range(2, 9)] + ['B8A', 'B11', 'B12', 'TCI_R', 'TCI_G', 'TCI_B']\n",
    "selected_bands = all_bands\n",
    "bands = [all_bands.index(b) for b in selected_bands]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d5926fbe-15c2-4fa6-8977-99eafdcb5ca3",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dir = Path('models')\n",
    "model_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "shards_dir = Path.home().joinpath('sentinel_data').joinpath('shards')\n",
    "\n",
    "feature_type = 'zscore_features' # zscore_features minmax_features\n",
    "\n",
    "label_type = 'selected_labels'\n",
    "sort_key = lambda x: int(x.stem.split('_')[-1])\n",
    "label_filepaths = sorted(list(shards_dir.joinpath(label_type).glob('label_*.npy')), key=sort_key)\n",
    "\n",
    "IDs = shuffle([int(f.stem.split('_')[-1]) for f in label_filepaths], random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "90c59975-e2bf-4311-a011-b7284efda661",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class DataGenerator(tf.keras.utils.PyDataset):\n",
    "    'Generates data for Keras'\n",
    "    def __init__(self, list_IDs, shuffle, **kwargs):\n",
    "        super().__init__()\n",
    "        vars(self).update(kwargs)\n",
    "        self.list_IDs = list_IDs\n",
    "        self.shuffle = shuffle\n",
    "        \n",
    "        self.use_multiprocessing = True\n",
    "        self.workers = 4\n",
    "        self.max_queue_size = 2\n",
    "        self.on_epoch_end()\n",
    "\n",
    "    def __len__(self):\n",
    "        'Number of batches per epoch.'\n",
    "        return int(np.floor(len(self.list_IDs) / self.batch_size))\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        'Get one batch of data.'\n",
    "        indexes = self.indexes[index*self.batch_size:(index+1)*self.batch_size]\n",
    "        list_IDs_temp = [self.list_IDs[k] for k in indexes]\n",
    "        X, y = self.data_generation(list_IDs_temp)\n",
    "        return X, y\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        'Update and shuffle indexes after each epoch.'\n",
    "        self.indexes = np.arange(len(self.list_IDs))\n",
    "        if self.shuffle == True:\n",
    "            np.random.shuffle(self.indexes)\n",
    "\n",
    "    def data_generation(self, list_IDs_temp):\n",
    "        'Generate batch.'\n",
    "        X = np.empty((self.batch_size, *self.dim, len(self.bands)))\n",
    "        y = np.empty((self.batch_size, self.n_classes))\n",
    "\n",
    "        for i, ID in enumerate(list_IDs_temp):\n",
    "            X[i,...] = np.load(self.shards_dir.joinpath(\n",
    "                'features').joinpath(f'feature_{ID}.npy'))[..., bands]\n",
    "            y[i] = np.load(self.shards_dir.joinpath(\n",
    "                self.label_type).joinpath(f'label_{ID}.npy'))\n",
    "            \n",
    "        return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7a5fb97e-b021-4620-9b31-468323822101",
   "metadata": {},
   "outputs": [],
   "source": [
    "class KerasModelCreator:\n",
    "    def __init__(self, **kwargs):\n",
    "        vars(self).update(kwargs)\n",
    "        self.kwargs = kwargs\n",
    "            \n",
    "    def display_logger(self, log_file, metrics):\n",
    "        metric_names = [m if isinstance(m, str) else m.name for m in metrics]\n",
    "        if log_file.is_file():\n",
    "            val_metrics = ['val_loss'] + ['val_' + x for x in metric_names]\n",
    "            df = pd.read_csv(log_file)[['epoch', 'loss'] + metric_names + val_metrics]\n",
    "            df['epoch'] += 1\n",
    "            print('Previous training:')\n",
    "            display(HTML(df.to_html(index=False)))\n",
    "    \n",
    "    def define_callbacks_and_logger(self, model_path, model_savepoint, log_file, metrics):\n",
    "        metric_names = [m if isinstance(m, str) else m.name for m in metrics]\n",
    "\n",
    "        callbacks = [\n",
    "            tf.keras.callbacks.BackupAndRestore(\n",
    "                model_savepoint, save_freq='epoch', delete_checkpoint=False\n",
    "            ),\n",
    "            tf.keras.callbacks.CSVLogger(log_file, append=True),\n",
    "            tf.keras.callbacks.ModelCheckpoint(\n",
    "                model_path, monitor='val_recall', save_best_only=True, \n",
    "                save_freq='epoch', initial_value_threshold=0.4,\n",
    "                verbose=0,\n",
    "            ),\n",
    "            tf.keras.callbacks.ReduceLROnPlateau(\n",
    "                monitor='val_recall', factor=0.9, patience=2, min_lr=3e-6,\n",
    "                verbose=1,\n",
    "            ),\n",
    "            tf.keras.callbacks.EarlyStopping(\n",
    "                monitor='val_recall', \n",
    "                verbose=1,\n",
    "                patience=10,\n",
    "                mode='max',\n",
    "                restore_best_weights=True\n",
    "            ),\n",
    "        ]\n",
    "        return callbacks\n",
    "    \n",
    "    def get_bias_and_weights(self):\n",
    "        all_labels = []\n",
    "        for ID in self.IDs:\n",
    "            f = self.shards_dir.joinpath(self.label_type).joinpath(f'label_{ID}.npy')\n",
    "            all_labels.append(np.load(f))\n",
    "        all_labels = np.vstack(all_labels)\n",
    "        neg, pos = np.bincount(all_labels.astype(int).flatten())\n",
    "        initial_bias = np.log([pos/neg])\n",
    "        class_weights = 0\n",
    "        return initial_bias, class_weights\n",
    "\n",
    "    def get_metrics(self):\n",
    "        prc = tf.keras.metrics.AUC(name='prc', curve='PR')\n",
    "\n",
    "        f1_scores = []\n",
    "        for average in ['micro', 'macro', 'weighted']:\n",
    "            f1_scores.append(\n",
    "                tf.keras.metrics.F1Score(\n",
    "                    average=average, threshold=0.5, name=f'{average}f1score')\n",
    "            )\n",
    "        metrics = [\n",
    "            'accuracy', 'recall', 'precision', 'auc', prc\n",
    "        ] + f1_scores\n",
    "\n",
    "        return metrics\n",
    "        \n",
    "    def run(self):\n",
    "        ''' \n",
    "        If not overwrite and there's an existing model, the model will \n",
    "        continue training if the given epoch is bigger than the previous,\n",
    "        else just evaluate.\n",
    "        Ensure train splits are the same across continuations / evaluations\n",
    "        by not modifying the random_state in split_and_normalise.\n",
    "        '''\n",
    "        model_savepoint = model_path.parent.joinpath(self.model_path.stem)\n",
    "        log_file = model_path.with_suffix('.log')\n",
    "        \n",
    "        metrics = self.get_metrics()\n",
    "\n",
    "        if self.overwrite:\n",
    "            for f in [model_path, log_file] + list(model_savepoint.glob('*')):\n",
    "                f.unlink(missing_ok=True)\n",
    "                \n",
    "        self.display_logger(log_file, metrics)\n",
    "        \n",
    "        callbacks = self.define_callbacks_and_logger(\n",
    "            model_path, model_savepoint, log_file, metrics)\n",
    "        \n",
    "        training_ids, test_ids = train_test_split(self.IDs, test_size=0.1, random_state=42)\n",
    "        validation_ids, test_ids = train_test_split(test_ids, test_size=0.9, random_state=42)\n",
    "\n",
    "        training_generator = DataGenerator(training_ids, shuffle=True, **self.kwargs)\n",
    "        testing_generator = DataGenerator(test_ids, shuffle=False, **self.kwargs)\n",
    "        validation_generator = DataGenerator(validation_ids, shuffle=False, **self.kwargs)\n",
    "        \n",
    "        if model_path.is_file():\n",
    "            print('Loading model...')\n",
    "            model = load_model(model_path)\n",
    "        else:\n",
    "            print('Calculating initial bias...', end=' ')\n",
    "            initial_bias, class_weights = self.get_bias_and_weights()\n",
    "            print(initial_bias)\n",
    "            print('Building model...')\n",
    "            model = self.build_model(\n",
    "                self.n_classes, (*self.dim, len(self.bands)), metrics,\n",
    "                self.architecture, self.loss,\n",
    "                output_bias=initial_bias,\n",
    "            )\n",
    "        print('Fitting...')\n",
    "        model.fit(\n",
    "            x=training_generator,\n",
    "            validation_data=validation_generator,\n",
    "            epochs=self.epochs,\n",
    "            callbacks=callbacks\n",
    "        )\n",
    "        return model, testing_generator\n",
    "\n",
    "    def build_vgg(self, input_layer):\n",
    "        x = input_layer\n",
    "        for filters in [self.base_filters, self.base_filters*2]:\n",
    "            for _ in range(2):\n",
    "                x = Conv2D(\n",
    "                    filters=filters, kernel_size=3, padding='same', activation='relu',\n",
    "                )(x)\n",
    "            x = MaxPooling2D(pool_size=2,strides=2)(x)\n",
    "            x = BatchNormalization()(x)\n",
    "\n",
    "        for filters in [self.base_filters*4, self.base_filters*8]:\n",
    "            for _ in range(3):\n",
    "                x = Conv2D(\n",
    "                    filters=filters, kernel_size=3, padding='same', activation='relu',\n",
    "                )(x)\n",
    "            x = MaxPooling2D(pool_size=2,strides=2)(x)\n",
    "            x = BatchNormalization()(x)\n",
    "\n",
    "        x = Flatten()(x)\n",
    "\n",
    "        for n_layers in range(2):\n",
    "            x = Dense(self.base_filters*64, activation='relu')(x)\n",
    "            x = Dropout(0.5)(x)\n",
    "        \n",
    "        return x\n",
    "    \n",
    "    def res_block(self, x, filters):\n",
    "        r = BatchNormalization()(x)\n",
    "        r = Activation('relu')(r)\n",
    "        r = Conv2D(\n",
    "            filters=filters, kernel_size=3, strides=2, padding='same', \n",
    "            kernel_initializer=glorot_uniform(seed=42)\n",
    "        )(r)\n",
    "        \n",
    "        r = BatchNormalization()(r)\n",
    "        r = Activation('relu')(r)\n",
    "        \n",
    "        r = Conv2D(\n",
    "            filters=filters, kernel_size=3, strides=1, padding='same', \n",
    "            kernel_initializer=glorot_uniform(seed=42)\n",
    "        )(r)\n",
    "        \n",
    "        r = Conv2D(\n",
    "            filters=1, kernel_size=1, strides=1, padding='valid'\n",
    "        )(r)\n",
    "        \n",
    "        x = Conv2D(\n",
    "            filters=filters, kernel_size=3, strides=2, padding='same', \n",
    "            kernel_initializer=glorot_uniform(seed=42)\n",
    "        )(x)\n",
    "        \n",
    "        return Add()([x, r])\n",
    "\n",
    "    def build_simple(self, input_layer):\n",
    "        x = Conv2D(\n",
    "            filters=self.base_filters, kernel_size=3, padding='same', activation='relu',\n",
    "        )(input_layer)\n",
    "        x = BatchNormalization()(x)\n",
    "\n",
    "        x = Flatten()(x)\n",
    "        \n",
    "        x = Dense(self.base_filters*8, activation='relu')(x)\n",
    "        return x\n",
    "\n",
    "    def build_resnet(self, input_layer):\n",
    "        x = self.res_block(input_layer, self.base_filters)\n",
    "        x = self.res_block(x, self.base_filters*2)\n",
    "        x = self.res_block(x, self.base_filters*4)\n",
    "        x = self.res_block(x, self.base_filters*8)\n",
    "        \n",
    "        x = Activation('relu')(x)\n",
    "        x = Flatten()(x)\n",
    "\n",
    "        x = Dense(self.base_filters*8, activation='relu')(x)\n",
    "        return x\n",
    "\n",
    "    def build_unet(self, input_layer):\n",
    "        nb_filter = [32, 64, 128, 256, 512]\n",
    "        \n",
    "        conv1 = Conv2D(32, 3, activation='elu', kernel_initializer='he_normal', padding='valid')(input_layer)\n",
    "        conv1 = Dropout(0.5)(conv1)\n",
    "        conv1 = Conv2D(32, 3, activation='elu', kernel_initializer='he_normal', padding='valid')(conv1)\n",
    "        conv1 = Dropout(0.5)(conv1)\n",
    "        pool1 = MaxPooling2D(2, strides=2)(conv1)\n",
    "        \n",
    "        conv2 = Conv2D(64, 3, activation='elu', kernel_initializer='he_normal', padding='same')(pool1)\n",
    "        conv2 = Dropout(0.5)(conv2)\n",
    "        conv2 = Conv2D(64, 3, activation='elu', kernel_initializer='he_normal', padding='same')(conv2)\n",
    "        conv2 = Dropout(0.5)(conv2)\n",
    "        pool2 = MaxPooling2D(2, strides=2)(conv2)\n",
    "        \n",
    "        upsample1_2 = Conv2DTranspose(nb_filter[0], 2, strides=2, padding='same')(conv2)\n",
    "        conv1_2 = concatenate([upsample1_2, conv1], axis=3)\n",
    "        conv3 = Conv2D(32, 3, activation='elu', kernel_initializer='he_normal', padding='same')(conv1_2)\n",
    "        conv3 = Dropout(0.5)(conv3)\n",
    "        conv3 = Conv2D(32, 3, activation='elu', kernel_initializer='he_normal', padding='same')(conv3)\n",
    "        conv3 = Dropout(0.5)(conv3)\n",
    "        \n",
    "        conv3_1 = Conv2D(128, 3, activation='elu', kernel_initializer='he_normal', padding='same')(pool2)\n",
    "        conv3_1 = Dropout(0.5)(conv3_1)\n",
    "        conv3_1 = Conv2D(128, 3, activation='elu', kernel_initializer='he_normal', padding='same')(conv3_1)\n",
    "        conv3_1 = Dropout(0.5)(conv3_1)\n",
    "        pool3 = MaxPooling2D(2, strides=2)(conv3_1)\n",
    "        \n",
    "        upsample2_2 = Conv2DTranspose(nb_filter[1], 2, strides=2, padding='same')(conv3_1)\n",
    "        conv2_2 = concatenate([upsample2_2, conv2], axis=3) #x10\n",
    "        conv2_2 = Conv2D(64, 3, activation='elu', kernel_initializer='he_normal', padding='same')(conv2_2)\n",
    "        conv2_2 = Dropout(0.5)(conv2_2)\n",
    "        conv2_2 = Conv2D(64, 3, activation='elu', kernel_initializer='he_normal', padding='same')(conv2_2)\n",
    "        conv2_2 = Dropout(0.5)(conv2_2)\n",
    "        \n",
    "        upsample1_3 = Conv2DTranspose(nb_filter[0], 2, strides=2, padding='same')(conv2_2)\n",
    "        conv1_3 = concatenate([upsample1_3, conv1, conv3], axis=3)\n",
    "        conv1_3 = Conv2D(32, 3, activation='elu', kernel_initializer='he_normal', padding='same')(conv1_3)\n",
    "        conv1_3 = Dropout(0.5)(conv1_3)\n",
    "        conv1_3 = Conv2D(32, 3, activation='elu', kernel_initializer='he_normal', padding='same')(conv1_3)\n",
    "        conv1_3 = Dropout(0.5)(conv1_3)\n",
    "        \n",
    "        conv4_1 = Conv2D(256, 3, activation='elu', kernel_initializer='he_normal', padding='same')(pool3)\n",
    "        conv4_1 = Dropout(0.5)(conv4_1)\n",
    "        conv4_1 = Conv2D(256, 3, activation='elu', kernel_initializer='he_normal', padding='same')(conv4_1)\n",
    "        conv4_1 = Dropout(0.5)(conv4_1)\n",
    "        pool4 = MaxPooling2D(2, strides=2)(conv4_1)\n",
    "        \n",
    "        upsample3_2 = Conv2DTranspose(nb_filter[2], 2, strides=2, padding='same')(conv4_1)\n",
    "        conv3_2 = concatenate([upsample3_2, conv3_1], axis=3)\n",
    "        conv3_2 = Conv2D(128, 3, activation='elu', kernel_initializer='he_normal', padding='same')(conv3_2)\n",
    "        conv3_2 = Dropout(0.5)(conv3_2)\n",
    "        conv3_2 = Conv2D(128, 3, activation='elu', kernel_initializer='he_normal', padding='same')(conv3_2)\n",
    "        conv3_2 = Dropout(0.5)(conv3_2)\n",
    "        \n",
    "        upsample2_3 = Conv2DTranspose(nb_filter[1], 2, strides=2, padding='same')(conv3_2)\n",
    "        conv2_3 = concatenate([upsample2_3, conv2, conv2_2], axis=3)\n",
    "        conv2_3 = Conv2D(64, 3, activation='elu', kernel_initializer='he_normal', padding='same')(conv2_3)\n",
    "        conv2_3 = Dropout(0.5)(conv2_3)\n",
    "        conv2_3 = Conv2D(64, 3, activation='elu', kernel_initializer='he_normal', padding='same')(conv2_3)\n",
    "        conv2_3 = Dropout(0.5)(conv2_3)\n",
    "        \n",
    "        upsample1_4 = Conv2DTranspose(nb_filter[0], 2, strides=2, padding='same')(conv2_3)\n",
    "        conv1_4 = concatenate([upsample1_4, conv1, conv3, conv1_3], axis=3)\n",
    "        conv1_4 = Conv2D(32, 3, activation='elu', kernel_initializer='he_normal', padding='same')(conv1_4)\n",
    "        conv1_4 = Dropout(0.5)(conv1_4)\n",
    "        conv1_4 = Conv2D(32, 3, activation='elu', kernel_initializer='he_normal', padding='same')(conv1_4)\n",
    "        conv1_4 = Dropout(0.5)(conv1_4)\n",
    "        \n",
    "        conv5_1 = Conv2D(512, 3, activation='elu', kernel_initializer='he_normal', padding='same')(pool4)\n",
    "        conv5_1 = Dropout(0.5)(conv5_1)\n",
    "        conv5_1 = Conv2D(512, 3, activation='elu', kernel_initializer='he_normal', padding='same')(conv5_1)\n",
    "        conv5_1 = Dropout(0.5)(conv5_1)\n",
    "        \n",
    "        upsample4_2 = Conv2DTranspose(nb_filter[3], 2, strides=2, padding='same')(conv5_1)\n",
    "        conv4_2 = concatenate([upsample4_2, conv4_1], axis=3) \n",
    "        conv4_2 = Conv2D(256, 3, activation='elu', kernel_initializer='he_normal', padding='same')(conv4_2)\n",
    "        conv4_2 = Dropout(0.5)(conv4_2)\n",
    "        conv4_2 = Conv2D(256, 3, activation='elu', kernel_initializer='he_normal', padding='same')(conv4_2)\n",
    "        conv4_2 = Dropout(0.5)(conv4_2)\n",
    "        \n",
    "        upsample3_3 = Conv2DTranspose(nb_filter[2], 2, strides=2, padding='same')(conv4_2)\n",
    "        conv3_3 = concatenate([upsample3_3, conv3_1, conv3_2], axis=3)\n",
    "        conv3_3 = Conv2D(128, 3, activation='elu', kernel_initializer='he_normal', padding='same')(conv3_3)\n",
    "        conv3_3 = Dropout(0.5)(conv3_3)\n",
    "        conv3_3 = Conv2D(128, 3, activation='elu', kernel_initializer='he_normal', padding='same')(conv3_3)\n",
    "        conv3_3 = Dropout(0.5)(conv3_3)\n",
    "        \n",
    "        upsample2_4 = Conv2DTranspose(nb_filter[1], 2, strides=2, padding='same')(conv3_3)\n",
    "        conv2_4 = concatenate([upsample2_4, conv2, conv2_2, conv2_3], axis=3)\n",
    "        conv2_4 = Conv2D(64, 3, activation='elu', kernel_initializer='he_normal', padding='same')(conv2_4)\n",
    "        conv2_4 = Dropout(0.5)(conv2_4)\n",
    "        conv2_4 = Conv2D(64, 3, activation='elu', kernel_initializer='he_normal', padding='same')(conv2_4)\n",
    "        conv2_4 = Dropout(0.5)(conv2_4)\n",
    "        \n",
    "        upsample1_5 = Conv2DTranspose(nb_filter[0], 2, strides=2, padding='same')(conv2_4)\n",
    "        conv1_5 = concatenate([upsample1_5, conv1, conv3, conv1_3, conv1_4], axis=3)\n",
    "        conv1_5 = Conv2D(32, 3, activation='elu', kernel_initializer='he_normal', padding='same')(conv1_5)\n",
    "        conv1_5 = Dropout(0.5)(conv1_5)\n",
    "        conv1_5 = Conv2D(32, 3, activation='elu', kernel_initializer='he_normal', padding='same')(conv1_5)\n",
    "        conv1_5 = Dropout(0.5)(conv1_5)\n",
    "\n",
    "        x = Flatten()(conv1_5)\n",
    "        x = Dense(self.base_filters*8, activation='relu')(x)      \n",
    "        return x\n",
    "\n",
    "    def topless_resnet50(self, input_shape, input_layer):\n",
    "        resnet = tf.keras.applications.ResNet50(\n",
    "            include_top=False,\n",
    "            weights=None,\n",
    "            input_shape=input_shape,\n",
    "            pooling=None\n",
    "        )\n",
    "        x = resnet(input_layer)\n",
    "        x = Flatten()(x)\n",
    "        x = Dense(self.base_filters*8, activation='relu')(x)\n",
    "        return x\n",
    "\n",
    "    def build_model(self, output_shape, input_shape, metrics, architecture, loss, output_bias=None):\n",
    "        if output_bias is not None:\n",
    "            output_bias = tf.keras.initializers.Constant(output_bias)\n",
    "         \n",
    "        input_layer = Input(input_shape)\n",
    "\n",
    "        match architecture.lower():\n",
    "            case 'simple':\n",
    "                x = self.build_simple(input_layer)\n",
    "            case 'unet':\n",
    "                x = self.build_unet(input_layer)\n",
    "            case 'resnet50':\n",
    "                x = self.topless_resnet50(input_shape, input_layer)\n",
    "            case 'resnet':\n",
    "                x = self.build_resnet(input_layer)\n",
    "            case 'vgg':\n",
    "                x = self.build_vgg(input_layer)\n",
    "        \n",
    "        outputs = Dense(output_shape, activation='sigmoid', bias_initializer=output_bias)(x)\n",
    "        \n",
    "        m = Model(inputs=input_layer, outputs=outputs)\n",
    "\n",
    "        adam = Adam(\n",
    "            learning_rate=0.001,\n",
    "            beta_1=0.9,\n",
    "            beta_2=0.999,\n",
    "            epsilon=1e-07,\n",
    "        )\n",
    "\n",
    "        m.compile(optimizer=adam, loss=loss, metrics=metrics)\n",
    "        \n",
    "        return m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "aadf50b8-eda2-4527-97d0-e4e81748f2ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "architecture = 'unet'\n",
    "loss = 'binary_focal_crossentropy'\n",
    "model_path = model_dir.joinpath(f'{architecture}-{loss}-{label_type}.keras')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f29e1273-d2c4-4722-9616-a17bf0a422d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating initial bias... [-2.57281661]\n",
      "Building model...\n",
      "Fitting...\n",
      "Epoch 1/30\n",
      "\u001b[1m 246/3344\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m15:35:47\u001b[0m 18s/step - accuracy: 0.0864 - auc: 0.5552 - loss: 5.0320 - macrof1score: 0.0394 - microf1score: 0.0485 - prc: 0.0886 - precision: 0.0714 - recall: 0.0529 - weightedf1score: 0.0492"
     ]
    }
   ],
   "source": [
    "params = dict(\n",
    "    dim=(100, 100),\n",
    "    shards_dir=shards_dir,\n",
    "    label_type=label_type,\n",
    "    IDs=IDs, \n",
    "    model_path=model_path,\n",
    "    n_classes=np.load(label_filepaths[0]).shape[0],\n",
    "    bands=bands,\n",
    "    architecture=architecture,\n",
    "    loss=loss,\n",
    "    batch_size=64,\n",
    "    base_filters=8,\n",
    "    epochs=30,\n",
    "    overwrite=True\n",
    ")\n",
    "model, testing_generator = KerasModelCreator(**params).run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f19411ca-97b2-45ee-a101-aaf2c87f7587",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_path = model_path.with_suffix('.eval.csv')\n",
    "if eval_path.is_file():\n",
    "    print(pd.read_csv(eval_path))\n",
    "else:\n",
    "    r = model.evaluate(x=testing_generator, verbose=1, return_dict=True)\n",
    "    df = pd.DataFrame.from_dict(r, orient='index', columns=['score'])\n",
    "    df.to_csv(eval_path)\n",
    "    print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77a57cfb-f044-4572-80c9-56b44f37a044",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_path = model_path.with_suffix('.preds.npy')\n",
    "if preds_path.is_file():\n",
    "    y_pred = np.load(preds_path)\n",
    "else:\n",
    "    y_pred = model.predict(x=testing_generator, verbose=1)\n",
    "    np.save(preds_path, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78dc75e3-1c10-4ddd-9c7e-246310b1bd02",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_path = model_path.with_suffix('.true.npy')\n",
    "total_batches = testing_generator.__len__()\n",
    "if labels_path.is_file():\n",
    "    y_true = np.load(labels_path)\n",
    "else:\n",
    "    y_true = []\n",
    "    iterator = tqdm(testing_generator, total=total_batches-1)\n",
    "    for i, (x, y) in enumerate(iterator):\n",
    "        y_true.append(y)\n",
    "        if i > total_batches - 2:\n",
    "            iterator.close()\n",
    "            break\n",
    "    y_true = np.vstack(y_true)\n",
    "    np.save(labels_path, y_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baa96622-fc8d-40ef-86c8-e9f3c5990163",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true.sum(), y_pred.round(0).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "189634b8-0482-4443-a909-6ec323fec15d",
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = confusion_matrix(y_true.flatten().astype(int), y_pred.round(0).flatten().astype(int))\n",
    "ConfusionMatrixDisplay(confusion_matrix=cm).plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "875120d1-0f74-4894-98e3-19e2d8adb8cf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ded61bd-6527-4b12-bd22-d3404e4a5040",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tf.keras.utils.plot_model(model, show_shapes=True)\n",
    "# model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb61ad19-349f-4c6b-ad12-fdc6a46117fc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "138d220c-0c2d-459a-b995-ec95e3036002",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8302c085-023a-41e8-b074-abfee088b461",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bc3620d-e7fc-47ee-8808-dcd77ac73ecd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "a = np.array([[1,1, 1], [0,1, 0]])\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0127d0bb-c632-4379-8b01-a86c0bd390f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "a.sum(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5ab704e-6c77-4671-9812-8af5b407bae4",
   "metadata": {},
   "outputs": [],
   "source": [
    "a[0].sum()/(a.shape[0] * np.bincount(a[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9592dc04-feeb-47bd-948c-9c74640fba5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "a[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f412b37e-ec0f-4f7d-83f8-4579213eec48",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.bincount(a[1])*2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5189141-5f07-4aac-a68f-00a24ec5d69d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import subprocess\n",
    "# subprocess.run(['sudo', 'shutdown', 'now'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea018983-f959-487b-9bda-61c90da0d6d9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "858b81a8-f8a7-4da4-8614-ba08abd93a0a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
