{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "45a09dcb-b90b-45d7-b5a2-54996a34ada0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-10 21:17:37.383404: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-06-10 21:17:38.515290: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "import datetime\n",
    "from pathlib import Path\n",
    "\n",
    "import intake\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.utils import shuffle\n",
    "import itertools\n",
    "\n",
    "from tensorflow.keras.regularizers import l2\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import backend as K\n",
    "\n",
    "from tensorflow.keras.layers import *\n",
    "from tensorflow.keras.optimizers import Adam, RMSprop, Nadam, SGD, AdamW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0749fbd3-b6ee-443c-b12c-741c2ee1f1bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_bands = [f'B{x}' for x in range(2, 9)] + ['B8A', 'B11', 'B12', 'TCI_R', 'TCI_G', 'TCI_B']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d5926fbe-15c2-4fa6-8977-99eafdcb5ca3",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dir = Path('models')\n",
    "model_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "data_dir = Path('/sentinel_data').joinpath('shards')\n",
    "\n",
    "sort_key = lambda x: int(x.stem.split('_')[-1])\n",
    "feature_filepaths = sorted(list(data_dir.glob('feature_*.npy')), key=sort_key)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a20ef66-8fee-41ab-9e7c-9269f0967b97",
   "metadata": {},
   "source": [
    "Adapted from https://stanford.edu/~shervine/blog/keras-how-to-generate-data-on-the-fly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "90c59975-e2bf-4311-a011-b7284efda661",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class DataGenerator(tf.keras.utils.Sequence):\n",
    "    'Generates data for Keras'\n",
    "    def __init__(self, list_IDs, data_dir, batch_size=32, dim=(100,100), n_channels=13,\n",
    "                 n_classes=242, shuffle=True, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.dim = dim\n",
    "        self.batch_size = batch_size\n",
    "        self.data_dir = data_dir\n",
    "        self.list_IDs = list_IDs\n",
    "        self.n_channels = n_channels\n",
    "        self.n_classes = n_classes\n",
    "        self.shuffle = shuffle\n",
    "        \n",
    "        self.on_epoch_end()\n",
    "\n",
    "    def __len__(self):\n",
    "        'Denotes the number of batches per epoch'\n",
    "        return int(np.floor(len(self.list_IDs) / self.batch_size))\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        'Generate one batch of data'\n",
    "        # Generate indexes of the batch\n",
    "        indexes = self.indexes[index*self.batch_size:(index+1)*self.batch_size]\n",
    "\n",
    "        # Find list of IDs\n",
    "        list_IDs_temp = [self.list_IDs[k] for k in indexes]\n",
    "\n",
    "        # Generate data\n",
    "        X, y = self.__data_generation(list_IDs_temp)\n",
    "\n",
    "        return X, y\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        'Updates indexes after each epoch'\n",
    "        self.indexes = np.arange(len(self.list_IDs))\n",
    "        if self.shuffle == True:\n",
    "            np.random.shuffle(self.indexes)\n",
    "\n",
    "    def __data_generation(self, list_IDs_temp):\n",
    "        'Generates data containing batch_size samples'\n",
    "        # Initialization\n",
    "        X = np.empty((self.batch_size, *self.dim, self.n_channels))\n",
    "        y = np.empty((self.batch_size, self.n_classes))\n",
    "\n",
    "        # Generate data\n",
    "        for i, ID in enumerate(list_IDs_temp):\n",
    "            # Store sample\n",
    "            X[i,] = np.load(data_dir.joinpath(f'feature_{ID}.npy'))\n",
    "            # Store class\n",
    "            y[i] = np.load(data_dir.joinpath(f'label_{ID}.npy'))\n",
    "        \n",
    "        return X, y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "333f6796-2ace-4bac-9b22-e50ef66e467f",
   "metadata": {},
   "source": [
    "F1 score adapted from https://medium.com/@matrixB/modified-cross-entropy-loss-for-multi-label-classification-with-class-a8afede21eb9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2f1ac513-b290-4593-a2ce-8ad67ef93cc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.utils.get_custom_objects().clear()\n",
    "@tf.keras.utils.register_keras_serializable()\n",
    "def custom_f1_score(y_true, y_pred):\n",
    "    y_true = tf.cast(y_true, dtype='float32')\n",
    "    \n",
    "    true_positives = K.sum(K.round(y_true * y_pred))\n",
    "    \n",
    "    possible_positives = K.sum(K.round(y_true))\n",
    "    \n",
    "    recall = true_positives / (possible_positives + K.epsilon())\n",
    "    \n",
    "    predicted_positives = K.sum(K.round(y_pred))\n",
    "    \n",
    "    precision = true_positives / (predicted_positives + K.epsilon())\n",
    "    \n",
    "    return (2 * precision * recall) / (precision + recall + K.epsilon())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7a5fb97e-b021-4620-9b31-468323822101",
   "metadata": {},
   "outputs": [],
   "source": [
    "class KerasModelCreator:\n",
    "    def topless_vgg(self, input_shape, output_shape, metrics):\n",
    "        m = tf.keras.Sequential()\n",
    "        vgg = tf.keras.applications.VGG16(\n",
    "            include_top=False,\n",
    "            weights=None,\n",
    "            input_shape=input_shape,\n",
    "            classes=output_shape,\n",
    "            classifier_activation='sigmoid',\n",
    "        )\n",
    "        m.add(vgg)\n",
    "        m.add(Flatten())\n",
    "        m.add(Dense(output_shape, activation='sigmoid'))\n",
    "        m.compile(optimizer='adam', loss='categorical_crossentropy', metrics=metrics)\n",
    "        return m\n",
    "    \n",
    "    def display_logger(self, log_file, metrics):\n",
    "        metric_names = [m if isinstance(m, str) else m.__name__ for m in metrics]\n",
    "        if log_file.is_file():\n",
    "            val_metrics = ['val_loss'] + ['val_' + x for x in metric_names]\n",
    "            df = pd.read_csv(log_file)[['epoch', 'loss'] + metric_names + val_metrics]\n",
    "            df['epoch'] += 1\n",
    "            print('Previous training:')\n",
    "            display(HTML(df.to_html(index=False)))\n",
    "    \n",
    "    def define_callbacks_and_logger(self, model_path, model_savepoint, log_file, metrics):\n",
    "        metric_names = [m if isinstance(m, str) else m.__name__ for m in metrics]\n",
    "\n",
    "        callbacks = [\n",
    "            tf.keras.callbacks.BackupAndRestore(\n",
    "                model_savepoint, save_freq='epoch', delete_checkpoint=False\n",
    "            ),\n",
    "            tf.keras.callbacks.CSVLogger(log_file, append=True),\n",
    "            tf.keras.callbacks.ModelCheckpoint(\n",
    "                model_path, monitor='val_accuracy', save_best_only=True, \n",
    "                save_freq='epoch', initial_value_threshold=0.4,\n",
    "                verbose=1,\n",
    "            ),\n",
    "            tf.keras.callbacks.ReduceLROnPlateau(\n",
    "                monitor='val_accuracy', factor=0.6, patience=2, min_lr=3e-6,\n",
    "                verbose=1,\n",
    "            ),\n",
    "        ]\n",
    "        return callbacks\n",
    "        \n",
    "    def build_model(self, output_shape, input_shape, metrics):\n",
    "        m = tf.keras.Sequential()\n",
    "        m.add(Input(input_shape))\n",
    "        \n",
    "        m.add(Conv2D(\n",
    "            filters=32, kernel_size=3, padding='valid', activation='relu',\n",
    "        ))\n",
    "        m.add(BatchNormalization())\n",
    "        \n",
    "        m.add(Flatten())\n",
    "        \n",
    "        m.add(Dense(64, activation='relu'))\n",
    "        m.add(BatchNormalization())\n",
    "        \n",
    "        m.add(Dense(output_shape, activation='sigmoid'))\n",
    "\n",
    "        m.compile(optimizer='adam', loss='binary_crossentropy', metrics=metrics)\n",
    "        \n",
    "        return m\n",
    "    \n",
    "    def run(self, IDs, model_path, batch_size=10, epochs=10, overwrite=False):\n",
    "        ''' \n",
    "        If not overwrite and there's an existing model, the model will \n",
    "        continue trainingw if the given epoch is bigger than the previous,\n",
    "        else just evaluate.\n",
    "        Ensure train splits are the same across continuations / evaluations\n",
    "        by not modifying the random_state in split_and_normalise.\n",
    "        '''\n",
    "        model_savepoint = model_path.parent.joinpath(model_path.stem)\n",
    "        log_file = model_path.with_suffix('.log')\n",
    " \n",
    "        metrics = ['accuracy', custom_f1_score, 'recall', 'precision', 'auc']       \n",
    "\n",
    "        if overwrite:\n",
    "            for f in [model_path, log_file] + list(model_savepoint.glob('*')):\n",
    "                f.unlink(missing_ok=True)\n",
    "                \n",
    "        self.display_logger(log_file, metrics)\n",
    "        \n",
    "        callbacks = self.define_callbacks_and_logger(\n",
    "            model_path, model_savepoint, log_file, metrics)\n",
    "        \n",
    "        params = {\n",
    "            'dim': (100, 100),\n",
    "            'batch_size': batch_size,\n",
    "            'n_classes': 242,\n",
    "            'n_channels': 13,\n",
    "            'shuffle': True\n",
    "        }\n",
    "        \n",
    "        \n",
    "        training_ids, validation_ids = train_test_split(IDs, train_size=0.9, random_state=42)\n",
    "        \n",
    "        training_generator = DataGenerator(training_ids, data_dir, **params)\n",
    "        validation_generator = DataGenerator(validation_ids, data_dir, **params)\n",
    "\n",
    "        if model_path.is_file():\n",
    "            model = tf.keras.models.load_model(model_path)\n",
    "        else:\n",
    "            model = self.build_model(params['n_classes'], (*params['dim'], params['n_channels']), metrics)\n",
    "\n",
    "        # model = self.topless_vgg(input_shape, num_classes, metrics)\n",
    "        \n",
    "        model.fit(\n",
    "            x=training_generator,\n",
    "            validation_data=validation_generator,\n",
    "            epochs=epochs,\n",
    "            callbacks=callbacks\n",
    "        )\n",
    "        \n",
    "        return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d105fc9d-03a1-418a-8deb-0d1b1975d7bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "parts = [f.stem.split('_')[-1] for f in feature_filepaths]\n",
    "model_name = f'conv_parts_{parts[0]}_to_{parts[-1]}.keras'\n",
    "model_path = model_dir.joinpath(model_name)\n",
    "\n",
    "IDs = [int(f.stem.split('_')[-1]) for f in feature_filepaths]\n",
    "IDs = shuffle(IDs, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f29e1273-d2c4-4722-9616-a17bf0a422d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Previous training:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>epoch</th>\n",
       "      <th>loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>custom_f1_score</th>\n",
       "      <th>recall</th>\n",
       "      <th>precision</th>\n",
       "      <th>auc</th>\n",
       "      <th>val_loss</th>\n",
       "      <th>val_accuracy</th>\n",
       "      <th>val_custom_f1_score</th>\n",
       "      <th>val_recall</th>\n",
       "      <th>val_precision</th>\n",
       "      <th>val_auc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.196390</td>\n",
       "      <td>0.707986</td>\n",
       "      <td>0.695076</td>\n",
       "      <td>0.81279</td>\n",
       "      <td>0.081795</td>\n",
       "      <td>0.962778</td>\n",
       "      <td>0.013933</td>\n",
       "      <td>0.897083</td>\n",
       "      <td>0.935285</td>\n",
       "      <td>0.879192</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.004631</td>\n",
       "      <td>0.867037</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.002081</td>\n",
       "      <td>0.876042</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.001433</td>\n",
       "      <td>0.866574</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000891</td>\n",
       "      <td>0.876875</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.000691</td>\n",
       "      <td>0.861505</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000482</td>\n",
       "      <td>0.876875</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.000397</td>\n",
       "      <td>0.857824</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000297</td>\n",
       "      <td>0.876875</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.000270</td>\n",
       "      <td>0.858495</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000222</td>\n",
       "      <td>0.876875</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.000207</td>\n",
       "      <td>0.861181</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000167</td>\n",
       "      <td>0.876875</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.000166</td>\n",
       "      <td>0.860602</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000140</td>\n",
       "      <td>0.876875</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.000140</td>\n",
       "      <td>0.859028</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000122</td>\n",
       "      <td>0.876875</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.000120</td>\n",
       "      <td>0.855301</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000106</td>\n",
       "      <td>0.876875</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.000107</td>\n",
       "      <td>0.855532</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000092</td>\n",
       "      <td>0.876875</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.000095</td>\n",
       "      <td>0.857755</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000084</td>\n",
       "      <td>0.876875</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.000087</td>\n",
       "      <td>0.854861</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000077</td>\n",
       "      <td>0.876875</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.000080</td>\n",
       "      <td>0.857454</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000071</td>\n",
       "      <td>0.876875</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.000074</td>\n",
       "      <td>0.856435</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000065</td>\n",
       "      <td>0.876875</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.000069</td>\n",
       "      <td>0.856898</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000062</td>\n",
       "      <td>0.876875</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-10 21:17:39.859450: W external/local_tsl/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 78675968 exceeds 10% of free system memory.\n",
      "2024-06-10 21:17:39.882037: W external/local_tsl/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 78675968 exceeds 10% of free system memory.\n",
      "2024-06-10 21:17:39.897713: W external/local_tsl/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 78675968 exceeds 10% of free system memory.\n",
      "2024-06-10 21:17:39.964638: W external/local_tsl/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 78675968 exceeds 10% of free system memory.\n",
      "2024-06-10 21:17:39.978913: W external/local_tsl/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 78675968 exceeds 10% of free system memory.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-10 21:18:03.345209: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:450] ShuffleDatasetV3:2: Filling up shuffle buffer (this may take a while): 4 of 8\n",
      "2024-06-10 21:18:17.923502: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:480] Shuffle buffer filled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m  5/432\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m22:59\u001b[0m 3s/step - accuracy: 0.8392 - auc: 1.0000 - custom_f1_score: 1.0000 - loss: 6.8272e-05 - precision: 1.0000 - recall: 1.0000"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "KerasModelCreator().run(\n",
    "    IDs, \n",
    "    model_path, \n",
    "    batch_size=100, \n",
    "    epochs=50, \n",
    "    overwrite=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ded61bd-6527-4b12-bd22-d3404e4a5040",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb61ad19-349f-4c6b-ad12-fdc6a46117fc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "138d220c-0c2d-459a-b995-ec95e3036002",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45e3e05a-a6f4-4957-9d4d-9523a113b06e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# keep_indices = np.where(y.sum(axis=1) > 0)[0]\n",
    "# y = y[keep_indices]\n",
    "# X = X[keep_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7244709f-4d2b-411e-9644-86ddcad03468",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1a88adb-eca0-44ba-9b83-49adb7994137",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tf.keras.models.load_model(model_path).summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8302c085-023a-41e8-b074-abfee088b461",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e476eaf-992b-4bb6-a973-de32c88805c4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5ab704e-6c77-4671-9812-8af5b407bae4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5189141-5f07-4aac-a68f-00a24ec5d69d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import subprocess\n",
    "# subprocess.run(['sudo', 'shutdown', 'now'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3d9ac97-f1ca-4de2-b1ff-9c02483c878e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea018983-f959-487b-9bda-61c90da0d6d9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
